{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv(\"../data/ner.csv\", encoding=\"latin1\")\n",
    "df = pd.read_csv(\"../data/ner_dataset.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordMapper(data):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "\n",
    "    vocab = list(set(data['Word'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "def TagMapper(data):\n",
    "    tag2idx = {}\n",
    "    idx2tag = {}\n",
    "\n",
    "    tags = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tag = {idx:tok for  idx, tok in enumerate(tags)}\n",
    "    tag2idx = {tok:idx for  idx, tok in enumerate(tags)}\n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "token2idx, idx2token = WordMapper(df)\n",
    "tag2idx, idx2tag = TagMapper(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WordIdx'] = df['Word'].map(token2idx)\n",
    "df['TagIdx'] = df['Tag'].map(tag2idx)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fillna = df.ffill(axis=0)\n",
    "data_fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = data_fillna.groupby(['Sentence #'], as_index=False).agg((lambda x: list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group.dropna(inplace=True)\n",
    "data_group.drop(['Sentence #'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in data_group['Word']:\n",
    "    if len(i) > count:\n",
    "        count = len(i)\n",
    "print(f\"maxlen {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104 layer input XD\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def PaddingTestTrainSplit(data_group, data):\n",
    "\n",
    "    UniqToken = len(token2idx)\n",
    "    UniqTag = len(tag2idx)\n",
    "    tokens = data_group['WordIdx'].tolist()\n",
    "    maxlen = 104\n",
    "    paddded = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= UniqToken-1)\n",
    "\n",
    "    #PAdding and OneHot Encoding\n",
    "    tags = data_group['TagIdx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    \n",
    "    pad_tags = [to_categorical(i, num_classes=UniqTag) for i in pad_tags]       #one hot encoding\n",
    "    \n",
    "    #Split test train\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(paddded, pad_tags, test_size=0.11, train_size=0.89, random_state=48)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.11,train_size =0.89, random_state=50)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = PaddingTestTrainSplit(data_group, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out which devices your operations and tensors are assigned to\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors and perform an operation\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)\n",
    "\n",
    "\"\"\"\n",
    "2.3.1\n",
    "Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
    "tf.Tensor(\n",
    "[[22. 28.]\n",
    " [49. 64.]], shape=(2, 2), dtype=float32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(104,)))\n",
    "    model.add(Embedding(input_dim=len(token2idx), output_dim=104, input_length=104))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(units=104, return_sequences=True, recurrent_dropout=0.1)))\n",
    "    model.add(TimeDistributed(Dense(len(tag2idx))))\n",
    "    model.add(Dense(len(tag2idx), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(df['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    adam = tf.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_bilstm_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(5):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "# model_mine = model_train()\n",
    "plot_model(model_bilstm_lstm)\n",
    "# plot_model(model_mine)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)\n",
    "# results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"London has such beautiful weather\"\n",
    "sentence = sentence.split()\n",
    "sentence = [token2idx[w] for w in sentence]\n",
    "sentence = np.array(sentence).reshape(1,-1)\n",
    "UniqToken = len(token2idx)\n",
    "UniqTag = len(tag2idx)\n",
    "tokens = sentence.tolist()\n",
    "print(tokens)\n",
    "maxlen = 104\n",
    "paddded = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= UniqToken-1)\n",
    "predictions = model.predict(paddded)\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "predictions = [idx2tag[e] for e in predictions[0]]\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
